{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40e1058",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05cb0644",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def bayes(train_x,train_y,data,trainWordCount,testing_data,unigram,vector,indexes2=None,):\n",
    "\n",
    "    indexes = []\n",
    "    for row in train_y.index:\n",
    "        indexes.append(row)\n",
    "\n",
    "    if(indexes2 == None):\n",
    "        for a,value in enumerate(train_x.toarray()):\n",
    "            category = data[\"Category\"][indexes[a]]\n",
    "            trainWordCount[category] = np.sum([trainWordCount[category],value],axis=0)\n",
    "        trainwordCountCopy = trainWordCount.copy()\n",
    "    else:\n",
    "        for a,value in enumerate(train_x.toarray()):\n",
    "            category = data[\"Category\"][indexes[a]]\n",
    "            temp = np.zeros(len(indexes2))\n",
    "            for count,b in enumerate(indexes2):\n",
    "                temp[count] = value[b]\n",
    "            trainWordCount[category] = np.sum([trainWordCount[category],temp],axis=0)\n",
    "\n",
    "        trainwordCountCopy = trainWordCount.copy()\n",
    "\n",
    "\n",
    "    bow = {}\n",
    "    if indexes2==None:\n",
    "        for a,value in enumerate(trainwordCountCopy):\n",
    "            bow[categories[a]] = {}\n",
    "            temp = {}\n",
    "            for b,val in enumerate(vector.get_feature_names_out()):\n",
    "                temp.update({val:trainwordCountCopy[a][b]})\n",
    "            bow[categories[a]] = temp\n",
    "    else:\n",
    "        for a,value in enumerate(trainwordCountCopy):\n",
    "            bow[categories[a]] = {}\n",
    "            temp = {}\n",
    "            countx = 0\n",
    "            for val in (indexes2):\n",
    "                temp.update({indexes2[val]:trainwordCountCopy[a][countx]})\n",
    "                countx += 1\n",
    "            bow[categories[a]] = temp\n",
    "    results = testing_data[\"Category\"]\n",
    "\n",
    "    (true,false) = 0,0\n",
    "    if(indexes2 == None):\n",
    "        constx = len(vector.get_feature_names_out())\n",
    "    else:\n",
    "        constx = len(indexes2)\n",
    "\n",
    "    time = datetime.datetime.now()\n",
    "    for count,a in enumerate(testing_data[\"Text\"]):\n",
    "        mx = -math.inf\n",
    "        maxindex = 0\n",
    "        for b in bow:\n",
    "            extend = 0\n",
    "            if len(bow) == 0:\n",
    "                print(\"bow:\",bow)\n",
    "            cons = math.log2(len(bow[b]) / len(train_y))\n",
    "            tr = np.sum(trainwordCountCopy[cat[b]])\n",
    "            if(unigram==False):\n",
    "                for c in range(len(a.split(\" \"))-1):\n",
    "                    word = str(a.split(\" \")[c])  + \" \" + str(a.split(\" \")[c+1])\n",
    "                    try:\n",
    "                        if bow[b][word] == 0:\n",
    "                            extend = constx\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        extend = constx\n",
    "                        break\n",
    "\n",
    "                for c in range(len(a.split(\" \"))-1):\n",
    "                    word = str(a.split(\" \")[c])  + \" \" + str(a.split(\" \")[c+1])\n",
    "                    try:\n",
    "                        number = bow[b][word]\n",
    "                        if(extend != 0):\n",
    "                            if bow[b][word] != 0:\n",
    "                                cons += math.log2((number +1) / (tr + extend))\n",
    "                            else:\n",
    "                                cons += math.log2((1) / (tr + extend))\n",
    "\n",
    "                        else:\n",
    "                            cons += math.log2((number) / (tr))\n",
    "                    except Exception:\n",
    "                        cons += math.log2((1) / (tr + extend))\n",
    "            else:\n",
    "                for c in a.split(\" \"):\n",
    "                    try:\n",
    "                        if bow[b][c] == 0:\n",
    "                            extend = constx\n",
    "                            break\n",
    "                    except Exception:\n",
    "                        extend = constx\n",
    "                        break\n",
    "                for c in a.split(\" \"):\n",
    "                    try:\n",
    "                        if c not in my_stop_words:\n",
    "                            if(extend != 0):\n",
    "                                if(bow[b][c] != 0):\n",
    "                                    cons += math.log2((bow[b][c] +1) / (tr + extend))\n",
    "                                else:\n",
    "                                    cons += math.log2((1) / (tr+extend))\n",
    "                            else:\n",
    "                                cons += math.log2((number) / (tr))\n",
    "                    except Exception:\n",
    "                        cons += math.log2((1)/(tr+extend))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            if(cons > mx):\n",
    "                mx = cons\n",
    "                maxindex = cat[b]\n",
    "        if(categories[maxindex] == categories[results[results.index[count]]]):\n",
    "            true += 1\n",
    "        else:\n",
    "            false += 1\n",
    "\n",
    "\n",
    "    \n",
    "    return true/(true+false)\n",
    "    \n",
    "\n",
    "def tfidf(vector):\n",
    "    specwords = []\n",
    "    df4=pd.DataFrame()\n",
    "    for a in range(5):\n",
    "        arr = []\n",
    "        corpus = training_data[training_data[\"Category\"] ==a][\"Text\"]\n",
    "        Z = vector.fit_transform(corpus)\n",
    "        vocabulary = vector.get_feature_names_out()\n",
    "        corpus = corpus.tolist()\n",
    "        pipe = Pipeline([('count', CountVectorizer(vocabulary=vocabulary)),('tfid', TfidfTransformer())]).fit(corpus)\n",
    "        idx = np.argpartition(pipe['tfid'].idf_, 10)\n",
    "        ls = pipe['tfid'].idf_\n",
    "        ls2 = pipe['tfid'].idf_\n",
    "        for b in range(10):\n",
    "            ind = np.argmin(ls)\n",
    "            value = ls[ind]\n",
    "            ls[ind] = math.inf\n",
    "            temp = [vocabulary[ind],value]\n",
    "            specwords.append(vocabulary[ind])\n",
    "            arr.append(temp)\n",
    "        df = pd.DataFrame(data = arr,\n",
    "                      columns = [\"Word\",\"TFID\"])\n",
    "        arr = []\n",
    "        \n",
    "        for b in range(10):\n",
    "            ind = np.argmax(ls2)\n",
    "            value = ls2[ind]\n",
    "            ls2[ind] = -math.inf\n",
    "            temp = [vocabulary[ind],value]\n",
    "            arr.append(temp)\n",
    "        df2 = pd.DataFrame(data = arr,\n",
    "                      columns = [\"Word\",\"TFID\"])\n",
    "\n",
    "        \n",
    "\n",
    "        res = []\n",
    "        for  m in specwords:\n",
    "            if m not in res:\n",
    "                res.append(m)\n",
    "    \n",
    "        df3 = pd.concat([df,df2],axis=1)\n",
    "        df3.columns=pd.MultiIndex.from_product([[categories[a].upper()],[\"Effect of Presence\",\"Effect of Absence\"],[\"Word\",\"TF-IDF\"]],names=[\"Category\",\"Effect\",\" \"])\n",
    "       \n",
    "        df4 = pd.concat([df4,df3],axis=1)\n",
    "    \n",
    "    return res,df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1943b83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"English Dataset.csv\")\n",
    "dataFrame = data.drop([\"ArticleId\"], axis=1)\n",
    "categories = {0:\"sport\",1:\"business\",2:\"politics\",3:\"entertainment\",4:\"tech\"}\n",
    "cat = {\"sport\":0,\"business\":1,\"politics\":2,\"entertainment\":3,\"tech\":4}\n",
    "dataFrame[\"Category\"] = dataFrame[\"Category\"].map(cat).astype(int)\n",
    "my_stop_words = text.ENGLISH_STOP_WORDS\n",
    "\n",
    "data = dataFrame.iloc[:, :]\n",
    "vectorizer = CountVectorizer(stop_words=\"english\")\n",
    "vectorizerwithStop = CountVectorizer()\n",
    "vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "vectorizer2withStop = CountVectorizer(stop_words=\"english\",analyzer='word', ngram_range=(2, 2))\n",
    "X = vectorizer.fit_transform(data[\"Text\"])\n",
    "\n",
    "wordCount = np.zeros((5,(X.toarray()).shape[1]))\n",
    "\n",
    "\n",
    "\n",
    "for a,value in enumerate(X.toarray()):\n",
    "    category = data[\"Category\"][a]\n",
    "    wordCount[category] = np.sum([wordCount[category],value],axis=0)\n",
    "\n",
    "wordCountCopy = wordCount.copy()\n",
    "wordnumberdf = pd.DataFrame()\n",
    "for count,a in enumerate(wordCountCopy):\n",
    "    dict = []\n",
    "    for b in range(4):\n",
    "        ind = np.argmax(a)\n",
    "        word = vectorizer.get_feature_names_out()[ind]\n",
    "        amount = int(a[ind])\n",
    "        temp = [word,amount]\n",
    "        dict.append(temp)\n",
    "        a[ind] = 0\n",
    "    \n",
    "    tempdf = pd.DataFrame(data = dict,\n",
    "                      columns = pd.MultiIndex.from_product([[categories[count].upper()],[\"Word\",\"Word Count\"]]))\n",
    "    \n",
    "    wordnumberdf=pd.concat([tempdf,wordnumberdf],axis=1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b8914cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(dataFrame, test_size=0.2, shuffle=True)\n",
    "\n",
    "train_x = vectorizer.fit_transform(training_data[\"Text\"])\n",
    "train_y = training_data[\"Category\"]\n",
    "train_x_withStop = vectorizerwithStop.fit_transform(training_data[\"Text\"])\n",
    "trainWordCount = np.zeros((5,(train_x.toarray()).shape[1]))\n",
    "trainWordCountwithStop = np.zeros((5,(train_x_withStop.toarray()).shape[1]))\n",
    "\n",
    "a1=bayes(train_x,train_y,data,trainWordCount,testing_data,True,vectorizer)\n",
    "\n",
    "b1=bayes(train_x_withStop,train_y,data,trainWordCountwithStop,testing_data,True,vectorizerwithStop)\n",
    "\n",
    "\n",
    "train_x_bigram = vectorizer2.fit_transform(training_data[\"Text\"])\n",
    "train_y_bigram = training_data[\"Category\"]\n",
    "train_x_bigram_stop=vectorizer2withStop.fit_transform(training_data[\"Text\"])\n",
    "trainWordCount_bigram = np.zeros((5,(train_x_bigram.toarray()).shape[1]))\n",
    "trainWordCount_bigram_stop=np.zeros((5,(train_x_bigram_stop.toarray()).shape[1]))\n",
    "\n",
    "accdf=pd.DataFrame(0.0,index=range(2), columns=range(2))\n",
    "\n",
    "\n",
    "c1=bayes(train_x_bigram,train_y_bigram,data,trainWordCount_bigram,testing_data,False,vectorizer2)\n",
    "d1=bayes(train_x_bigram_stop,train_y_bigram,data,trainWordCount_bigram_stop,testing_data,False,vectorizerwithStop)\n",
    "\n",
    "\n",
    "\n",
    "vectorizer3 = CountVectorizer()\n",
    "newdata,tfidfWithStopword = tfidf(vectorizer3)\n",
    "train_x_updated_1 = vectorizer3.fit_transform(training_data[\"Text\"])\n",
    "train_y_updated_1 = training_data[\"Category\"]\n",
    "\n",
    "vectorizer4 = CountVectorizer(stop_words=\"english\")\n",
    "\n",
    "newdata2,tfidfWithoutStopword = tfidf(vectorizer4)\n",
    "train_x_updated_2 = vectorizer4.fit_transform(training_data[\"Text\"])\n",
    "train_y_updated_2 = training_data[\"Category\"]\n",
    "\n",
    "(indexes3,indexes4)  = ({},{})\n",
    "for count,a in enumerate(vectorizer3.get_feature_names_out()):\n",
    "    if a in newdata:\n",
    "        indexes3[count] = a\n",
    "\n",
    "\n",
    "for count,a in enumerate(vectorizer4.get_feature_names_out()):\n",
    "    if a in newdata2:\n",
    "        indexes4[count] = a\n",
    "\n",
    "newCountArr = np.zeros((5,(len(indexes3))))\n",
    "newCountArr2 = np.zeros((5,(len(indexes4))))\n",
    "e1=bayes(train_x_updated_1,train_y_updated_1,data,newCountArr,testing_data,True,vectorizer3,indexes3)\n",
    "f1=bayes(train_x_updated_2,train_y_updated_2,data,newCountArr2,testing_data,True,vectorizer4,indexes4)\n",
    "\n",
    "vectorizer5 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "vectorizer6withStop = CountVectorizer(stop_words=\"english\",analyzer='word', ngram_range=(2, 2))\n",
    "\n",
    "accdf[0][0]=a1\n",
    "accdf[1][0]=b1\n",
    "accdf[0][1]=c1\n",
    "accdf[1][1]=d1\n",
    "\n",
    "accdf.index =[\"Unigram\",\"Bigram\"]\n",
    "accdf.columns=pd.MultiIndex.from_product([[\"Included\",\"Not Included\"]],names = [\"Stop-word\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857180df",
   "metadata": {},
   "source": [
    "# Report\n",
    "## Part I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1200d093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">TECH</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ENTERTAINMENT</th>\n",
       "      <th colspan=\"2\" halign=\"left\">POLITICS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">BUSINESS</th>\n",
       "      <th colspan=\"2\" halign=\"left\">SPORT</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Word</th>\n",
       "      <th>Word Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>1064</td>\n",
       "      <td>said</td>\n",
       "      <td>594</td>\n",
       "      <td>said</td>\n",
       "      <td>1445</td>\n",
       "      <td>said</td>\n",
       "      <td>1100</td>\n",
       "      <td>said</td>\n",
       "      <td>636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>647</td>\n",
       "      <td>film</td>\n",
       "      <td>583</td>\n",
       "      <td>mr</td>\n",
       "      <td>1073</td>\n",
       "      <td>year</td>\n",
       "      <td>456</td>\n",
       "      <td>game</td>\n",
       "      <td>356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mr</td>\n",
       "      <td>349</td>\n",
       "      <td>best</td>\n",
       "      <td>430</td>\n",
       "      <td>labour</td>\n",
       "      <td>494</td>\n",
       "      <td>mr</td>\n",
       "      <td>393</td>\n",
       "      <td>year</td>\n",
       "      <td>331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>new</td>\n",
       "      <td>349</td>\n",
       "      <td>year</td>\n",
       "      <td>315</td>\n",
       "      <td>government</td>\n",
       "      <td>464</td>\n",
       "      <td>market</td>\n",
       "      <td>284</td>\n",
       "      <td>england</td>\n",
       "      <td>329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     TECH            ENTERTAINMENT               POLITICS            BUSINESS  \\\n",
       "     Word Word Count          Word Word Count        Word Word Count     Word   \n",
       "0    said       1064          said        594        said       1445     said   \n",
       "1  people        647          film        583          mr       1073     year   \n",
       "2      mr        349          best        430      labour        494       mr   \n",
       "3     new        349          year        315  government        464   market   \n",
       "\n",
       "                SPORT             \n",
       "  Word Count     Word Word Count  \n",
       "0       1100     said        636  \n",
       "1        456     game        356  \n",
       "2        393     year        331  \n",
       "3        284  england        329  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordnumberdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251827cb",
   "metadata": {},
   "source": [
    "Guessing a category from the headline is possible for some categories but the accuracy of guessing by looking at a single word is low. For example, if we see \"people\", we can say it may be tech but, the most repeated word except the stop words in each category is \"said\". When we see \"said\" in headline , category may be anyone of categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f9b14e",
   "metadata": {},
   "source": [
    "### Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27a8db15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Stop-word</th>\n",
       "      <th>Included</th>\n",
       "      <th>Not Included</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Unigram</th>\n",
       "      <td>0.986577</td>\n",
       "      <td>0.983221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bigram</th>\n",
       "      <td>0.976510</td>\n",
       "      <td>0.161074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Stop-word  Included Not Included\n",
       "Unigram    0.986577     0.983221\n",
       "Bigram     0.976510     0.161074"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3442a34b",
   "metadata": {},
   "source": [
    "## Part II\n",
    "First of all, deleting stop words doesn't cause big changes. Because all of the categories\n",
    "have lots of stop words. It can cause small changes. Secondly, unigram's accuracy and bigram's accuracy\n",
    "are very close too. Both of them specify the categories well. Average accuracy is 0.95. The accuracy\n",
    "depends on the training data/test data which was shuffled. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39808234",
   "metadata": {},
   "source": [
    "### • Table of the most and the least influential words, including stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e947720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SPORT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BUSINESS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">POLITICS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ENTERTAINMENT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">TECH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Effect</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>of</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>000bn</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>in</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>the</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>001st</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>and</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>00</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>to</td>\n",
       "      <td>1.003604</td>\n",
       "      <td>01</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>the</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>01</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>the</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0001</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>and</td>\n",
       "      <td>1.004435</td>\n",
       "      <td>007</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>the</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0051</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>of</td>\n",
       "      <td>1.014493</td>\n",
       "      <td>04</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>to</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>03</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>to</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>05</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>in</td>\n",
       "      <td>1.004435</td>\n",
       "      <td>0400</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>to</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>007</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>and</td>\n",
       "      <td>1.018149</td>\n",
       "      <td>05</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>in</td>\n",
       "      <td>1.003683</td>\n",
       "      <td>04</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>and</td>\n",
       "      <td>1.004598</td>\n",
       "      <td>050505</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>of</td>\n",
       "      <td>1.008889</td>\n",
       "      <td>056</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>of</td>\n",
       "      <td>1.009901</td>\n",
       "      <td>028</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>in</td>\n",
       "      <td>1.018149</td>\n",
       "      <td>05m</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>and</td>\n",
       "      <td>1.007380</td>\n",
       "      <td>041</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>of</td>\n",
       "      <td>1.004598</td>\n",
       "      <td>06</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>to</td>\n",
       "      <td>1.022372</td>\n",
       "      <td>0700</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>in</td>\n",
       "      <td>1.019901</td>\n",
       "      <td>056</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>for</td>\n",
       "      <td>1.078545</td>\n",
       "      <td>060</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>said</td>\n",
       "      <td>1.088349</td>\n",
       "      <td>04bn</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>for</td>\n",
       "      <td>1.051776</td>\n",
       "      <td>072</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>on</td>\n",
       "      <td>1.073427</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>that</td>\n",
       "      <td>1.019901</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>but</td>\n",
       "      <td>1.134560</td>\n",
       "      <td>093</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>for</td>\n",
       "      <td>1.096414</td>\n",
       "      <td>050</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>is</td>\n",
       "      <td>1.051776</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>for</td>\n",
       "      <td>1.082997</td>\n",
       "      <td>0845</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>it</td>\n",
       "      <td>1.024939</td>\n",
       "      <td>102</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>at</td>\n",
       "      <td>1.142824</td>\n",
       "      <td>09secs</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>that</td>\n",
       "      <td>1.121005</td>\n",
       "      <td>06</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>on</td>\n",
       "      <td>1.056619</td>\n",
       "      <td>09</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>was</td>\n",
       "      <td>1.157845</td>\n",
       "      <td>0900</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>is</td>\n",
       "      <td>1.035091</td>\n",
       "      <td>104</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>on</td>\n",
       "      <td>1.159561</td>\n",
       "      <td>1000m</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>is</td>\n",
       "      <td>1.125163</td>\n",
       "      <td>069</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>be</td>\n",
       "      <td>1.066375</td>\n",
       "      <td>100bn</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>has</td>\n",
       "      <td>1.189426</td>\n",
       "      <td>102</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>for</td>\n",
       "      <td>1.040206</td>\n",
       "      <td>106</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>with</td>\n",
       "      <td>1.193899</td>\n",
       "      <td>102</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>it</td>\n",
       "      <td>1.133531</td>\n",
       "      <td>07</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>said</td>\n",
       "      <td>1.071289</td>\n",
       "      <td>100m</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>with</td>\n",
       "      <td>1.194788</td>\n",
       "      <td>103</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>be</td>\n",
       "      <td>1.071459</td>\n",
       "      <td>1080</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category              SPORT                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                       the  1.000000                00  5.934474   \n",
       "1                        to  1.003604                01  5.934474   \n",
       "2                        of  1.014493                04  5.934474   \n",
       "3                       and  1.018149                05  5.934474   \n",
       "4                        in  1.018149               05m  5.934474   \n",
       "5                       for  1.078545               060  5.934474   \n",
       "6                       but  1.134560               093  5.934474   \n",
       "7                        at  1.142824            09secs  5.934474   \n",
       "8                        on  1.159561             1000m  5.934474   \n",
       "9                      with  1.193899               102  5.934474   \n",
       "\n",
       "Category           BUSINESS                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                        of  1.000000             000bn  5.912655   \n",
       "1                       the  1.000000                01  5.912655   \n",
       "2                        to  1.000000                03  5.912655   \n",
       "3                        in  1.003683                04  5.912655   \n",
       "4                       and  1.007380               041  5.912655   \n",
       "5                      said  1.088349              04bn  5.912655   \n",
       "6                       for  1.096414               050  5.912655   \n",
       "7                      that  1.121005                06  5.912655   \n",
       "8                        is  1.125163               069  5.912655   \n",
       "9                        it  1.133531                07  5.912655   \n",
       "\n",
       "Category           POLITICS                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                        in  1.000000                00  5.691348   \n",
       "1                       the  1.000000              0001  5.691348   \n",
       "2                        to  1.000000                05  5.691348   \n",
       "3                       and  1.004598            050505  5.691348   \n",
       "4                        of  1.004598                06  5.691348   \n",
       "5                       for  1.051776               072  5.691348   \n",
       "6                        is  1.051776              0800  5.691348   \n",
       "7                        on  1.056619                09  5.691348   \n",
       "8                        be  1.066375             100bn  5.691348   \n",
       "9                      said  1.071289              100m  5.691348   \n",
       "\n",
       "Category      ENTERTAINMENT                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                       the  1.000000             001st  5.727388   \n",
       "1                       and  1.004435               007  5.727388   \n",
       "2                        in  1.004435              0400  5.727388   \n",
       "3                        of  1.008889               056  5.727388   \n",
       "4                        to  1.022372              0700  5.727388   \n",
       "5                        on  1.073427              0800  5.727388   \n",
       "6                       for  1.082997              0845  5.727388   \n",
       "7                       was  1.157845              0900  5.727388   \n",
       "8                       has  1.189426               102  5.727388   \n",
       "9                      with  1.194788               103  5.727388   \n",
       "\n",
       "Category               TECH                                        \n",
       "Effect   Effect of Presence           Effect of Absence            \n",
       "                       Word    TF-IDF              Word    TF-IDF  \n",
       "0                       and  1.000000                00  5.620059  \n",
       "1                       the  1.000000              0051  5.620059  \n",
       "2                        to  1.000000               007  5.620059  \n",
       "3                        of  1.009901               028  5.620059  \n",
       "4                        in  1.019901               056  5.620059  \n",
       "5                      that  1.019901              0800  5.620059  \n",
       "6                        it  1.024939               102  5.620059  \n",
       "7                        is  1.035091               104  5.620059  \n",
       "8                       for  1.040206               106  5.620059  \n",
       "9                        be  1.071459              1080  5.620059  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfWithStopword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45453385",
   "metadata": {},
   "source": [
    "### • Table of the most and the least influential words, except stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e254cebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Category</th>\n",
       "      <th colspan=\"4\" halign=\"left\">SPORT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">BUSINESS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">POLITICS</th>\n",
       "      <th colspan=\"4\" halign=\"left\">ENTERTAINMENT</th>\n",
       "      <th colspan=\"4\" halign=\"left\">TECH</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Effect</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Presence</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Effect of Absence</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Word</th>\n",
       "      <th>TF-IDF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>said</td>\n",
       "      <td>1.271035</td>\n",
       "      <td>00</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>said</td>\n",
       "      <td>1.088349</td>\n",
       "      <td>000bn</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>said</td>\n",
       "      <td>1.071289</td>\n",
       "      <td>00</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>said</td>\n",
       "      <td>1.284737</td>\n",
       "      <td>001st</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>said</td>\n",
       "      <td>1.114709</td>\n",
       "      <td>00</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>year</td>\n",
       "      <td>1.616986</td>\n",
       "      <td>01</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>year</td>\n",
       "      <td>1.458308</td>\n",
       "      <td>01</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>mr</td>\n",
       "      <td>1.254596</td>\n",
       "      <td>0001</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>year</td>\n",
       "      <td>1.450722</td>\n",
       "      <td>007</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>people</td>\n",
       "      <td>1.269781</td>\n",
       "      <td>0051</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>time</td>\n",
       "      <td>1.678861</td>\n",
       "      <td>04</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>new</td>\n",
       "      <td>1.810012</td>\n",
       "      <td>03</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>government</td>\n",
       "      <td>1.394062</td>\n",
       "      <td>05</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>film</td>\n",
       "      <td>1.738404</td>\n",
       "      <td>0400</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>new</td>\n",
       "      <td>1.461176</td>\n",
       "      <td>007</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>win</td>\n",
       "      <td>1.714966</td>\n",
       "      <td>05</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>market</td>\n",
       "      <td>1.843628</td>\n",
       "      <td>04</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>people</td>\n",
       "      <td>1.588705</td>\n",
       "      <td>050505</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>new</td>\n",
       "      <td>1.795562</td>\n",
       "      <td>056</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>mr</td>\n",
       "      <td>1.698085</td>\n",
       "      <td>028</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>game</td>\n",
       "      <td>1.823600</td>\n",
       "      <td>05m</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>company</td>\n",
       "      <td>1.852212</td>\n",
       "      <td>041</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>minister</td>\n",
       "      <td>1.622321</td>\n",
       "      <td>06</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>best</td>\n",
       "      <td>1.966188</td>\n",
       "      <td>0700</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>use</td>\n",
       "      <td>1.718086</td>\n",
       "      <td>056</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>world</td>\n",
       "      <td>1.945490</td>\n",
       "      <td>060</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>firm</td>\n",
       "      <td>1.852212</td>\n",
       "      <td>04bn</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>labour</td>\n",
       "      <td>1.665996</td>\n",
       "      <td>072</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>star</td>\n",
       "      <td>2.038508</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>year</td>\n",
       "      <td>1.738495</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>just</td>\n",
       "      <td>1.973661</td>\n",
       "      <td>093</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>mr</td>\n",
       "      <td>2.010682</td>\n",
       "      <td>050</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>new</td>\n",
       "      <td>1.730535</td>\n",
       "      <td>0800</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>uk</td>\n",
       "      <td>2.116470</td>\n",
       "      <td>0845</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>make</td>\n",
       "      <td>1.759329</td>\n",
       "      <td>102</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>team</td>\n",
       "      <td>2.002648</td>\n",
       "      <td>09secs</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>chief</td>\n",
       "      <td>2.041454</td>\n",
       "      <td>06</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>election</td>\n",
       "      <td>1.740104</td>\n",
       "      <td>09</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>including</td>\n",
       "      <td>2.130076</td>\n",
       "      <td>0900</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>way</td>\n",
       "      <td>1.759329</td>\n",
       "      <td>104</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>won</td>\n",
       "      <td>2.042654</td>\n",
       "      <td>1000m</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>2004</td>\n",
       "      <td>2.051925</td>\n",
       "      <td>069</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>party</td>\n",
       "      <td>1.759522</td>\n",
       "      <td>100bn</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>time</td>\n",
       "      <td>2.130076</td>\n",
       "      <td>102</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>like</td>\n",
       "      <td>1.769911</td>\n",
       "      <td>106</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>old</td>\n",
       "      <td>2.084326</td>\n",
       "      <td>102</td>\n",
       "      <td>5.934474</td>\n",
       "      <td>years</td>\n",
       "      <td>2.117166</td>\n",
       "      <td>07</td>\n",
       "      <td>5.912655</td>\n",
       "      <td>told</td>\n",
       "      <td>1.779325</td>\n",
       "      <td>100m</td>\n",
       "      <td>5.691348</td>\n",
       "      <td>years</td>\n",
       "      <td>2.130076</td>\n",
       "      <td>103</td>\n",
       "      <td>5.727388</td>\n",
       "      <td>technology</td>\n",
       "      <td>1.791417</td>\n",
       "      <td>1080</td>\n",
       "      <td>5.620059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Category              SPORT                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                      said  1.271035                00  5.934474   \n",
       "1                      year  1.616986                01  5.934474   \n",
       "2                      time  1.678861                04  5.934474   \n",
       "3                       win  1.714966                05  5.934474   \n",
       "4                      game  1.823600               05m  5.934474   \n",
       "5                     world  1.945490               060  5.934474   \n",
       "6                      just  1.973661               093  5.934474   \n",
       "7                      team  2.002648            09secs  5.934474   \n",
       "8                       won  2.042654             1000m  5.934474   \n",
       "9                       old  2.084326               102  5.934474   \n",
       "\n",
       "Category           BUSINESS                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                      said  1.088349             000bn  5.912655   \n",
       "1                      year  1.458308                01  5.912655   \n",
       "2                       new  1.810012                03  5.912655   \n",
       "3                    market  1.843628                04  5.912655   \n",
       "4                   company  1.852212               041  5.912655   \n",
       "5                      firm  1.852212              04bn  5.912655   \n",
       "6                        mr  2.010682               050  5.912655   \n",
       "7                     chief  2.041454                06  5.912655   \n",
       "8                      2004  2.051925               069  5.912655   \n",
       "9                     years  2.117166                07  5.912655   \n",
       "\n",
       "Category           POLITICS                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                      said  1.071289                00  5.691348   \n",
       "1                        mr  1.254596              0001  5.691348   \n",
       "2                government  1.394062                05  5.691348   \n",
       "3                    people  1.588705            050505  5.691348   \n",
       "4                  minister  1.622321                06  5.691348   \n",
       "5                    labour  1.665996               072  5.691348   \n",
       "6                       new  1.730535              0800  5.691348   \n",
       "7                  election  1.740104                09  5.691348   \n",
       "8                     party  1.759522             100bn  5.691348   \n",
       "9                      told  1.779325              100m  5.691348   \n",
       "\n",
       "Category      ENTERTAINMENT                                        \\\n",
       "Effect   Effect of Presence           Effect of Absence             \n",
       "                       Word    TF-IDF              Word    TF-IDF   \n",
       "0                      said  1.284737             001st  5.727388   \n",
       "1                      year  1.450722               007  5.727388   \n",
       "2                      film  1.738404              0400  5.727388   \n",
       "3                       new  1.795562               056  5.727388   \n",
       "4                      best  1.966188              0700  5.727388   \n",
       "5                      star  2.038508              0800  5.727388   \n",
       "6                        uk  2.116470              0845  5.727388   \n",
       "7                 including  2.130076              0900  5.727388   \n",
       "8                      time  2.130076               102  5.727388   \n",
       "9                     years  2.130076               103  5.727388   \n",
       "\n",
       "Category               TECH                                        \n",
       "Effect   Effect of Presence           Effect of Absence            \n",
       "                       Word    TF-IDF              Word    TF-IDF  \n",
       "0                      said  1.114709                00  5.620059  \n",
       "1                    people  1.269781              0051  5.620059  \n",
       "2                       new  1.461176               007  5.620059  \n",
       "3                        mr  1.698085               028  5.620059  \n",
       "4                       use  1.718086               056  5.620059  \n",
       "5                      year  1.738495              0800  5.620059  \n",
       "6                      make  1.759329               102  5.620059  \n",
       "7                       way  1.759329               104  5.620059  \n",
       "8                      like  1.769911               106  5.620059  \n",
       "9                technology  1.791417              1080  5.620059  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfWithoutStopword"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55d77b90",
   "metadata": {},
   "source": [
    "## Part III\n",
    "In this part,we want to find 10 words whose presence most strongly predicts that the article belongs to specific category, and 10 words whose absence most strongly predicts that the article belongs to specific category. We need Term Frequency - Inverce Document Frequency to do that. After calculating tf-idf value for all the words, the presence of words have lowest tf-idf value  most strongly predicts the article belongs to specific category. Predictably, the absence of words have highest tf-idf value most strongly predicts the article belongs to specific category. When we do this without deleting stop words, we can see the accuracy is too low. Its average is 0.16-0.20 because almost all stop words exist in each category. This makes predicting too hard.It is like predicting randomly, and the accuracy is very close to 1/5. When we delete the stop words, accuracy increases. Because we get the most 10 important words of each category, but it is not enough too. The average of accuracy is 0.30-0.35 . It's reason is texts have a lot of words, limiting them with low size words mean that probability of predicting correct is low."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227d51b7",
   "metadata": {},
   "source": [
    "Accuracy of collapsed data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "26e2a7bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1610738255033557"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90c1b652",
   "metadata": {},
   "source": [
    "Accuracy of collapsed data except stop-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fa87d228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31543624161073824"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d470121",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
